{
    "content": "Google DeepMind\u2019s AI systems have taken big scientific strides in recent years \u2014 from predicting the 3D structures of almost every known protein in the universe to forecasting weather more accurately than ever before.\n\nThe UK-based lab today unveiled its latest advancement: AlphaEvolve, an AI coding agent that makes large language models (LLMs) like Gemini better at solving complex computing and mathematical problems.\n\nAlphaEvolve is powered by the same models that it\u2019s trying to improve. Using Gemini, the agent proposes programs \u2014 written in code \u2014 that try to solve a given problem. It runs each code snippet through automated tests that evaluate how accurate, efficient, or novel it is. AlphaEvolve keeps the top-performing code snippets and uses them as the basis for the next round of generation. Over many cycles, this process \u201cevolves\u201d better and better solutions. In essence, it is a self-evolving AI.\n\nDeepMind has already used AlphaEvolve to tackle data centre energy use, design better chips, and speed up AI training. Here are five of its top feats so far.\n\n1. It discovered new solutions to some of the world\u2019s toughest maths problems\n\nAlphaEvolve was put to the test on over 50 open problems in maths, from combinatorics to number theory. In 20% of cases, it improved on the best-known solutions to them.\n\nOne of those was the 300-year-old kissing number problem. In 11-dimensional space, AlphaEvolve discovered a new lower bound with a configuration of 593 spheres \u2014 progress that even expert mathematicians hadn\u2019t reached.\n\n2. It made Google\u2019s data centres more efficient\n\nThe AI agent devised a way to better manage power scheduling at Google\u2019s data centres. That has allowed the tech giant to improve its data centre energy efficiency by 0.7% over the last year \u2014 a significant cost and energy saver given the size of its data centre operation.\n\n3. It helped train Gemini faster\n\nAlphaEvolve improved the way matrix multiplications are split into subproblems, a core operation in training AI models like Gemini. That optimisation sped up the process by 23%, reducing Gemini\u2019s total training time by 1%. In the world of generative AI, every percentage point can translate into cost and energy savings.\n\n4. It co-designed part of Google\u2019s next AI chip\n\nThe agent is also using its code-writing skills to rewire things in the physical world. It rewrote a portion of an arithmetic circuit in Verilog \u2014 a language used for chip design \u2014 making it more efficient. That same logic is now being used to develop Google\u2019s future TPU (Tensor Processing Unit), an advanced chip for machine learning.\n\n5. It beat a legendary algorithm from 1969\n\nFor decades, Strassen\u2019s algorithm was the gold standard for multiplying 4\u00d74 complex matrices. AlphaEvolve found a more efficient solution \u2014 using fewer scalar multiplications. This could lead to more advanced LLMs, which rely heavily on matrix multiplication to function.\n\nAccording to DeepMind, these feats are just the tip of the iceberg for AlphaEvolve. The lab envisions the agent solving countless problems, from discovering new materials and drugs to streamlining business operations.\n\nAI\u2019s evolution will be a hot topic at TNW Conference, which takes place on June 19-20 in Amsterdam. Tickets for the event are now on sale \u2014 use the code TNWXMEDIA2025 at the checkout to get 30% off.",
    "title": "5 impressive feats of DeepMind\u2019s new self-evolving AI coding agent",
    "publish_date": "2025-05-14"
}