{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b83927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a06986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: RUNID_3 at 2025-06-03 12:57:07\n"
     ]
    }
   ],
   "source": [
    "TASK_NAME = \"source_parsing_v0\"\n",
    "\n",
    "\n",
    "def get_run_id():\n",
    "    return os.getenv('RUNID') \n",
    "\n",
    "RUNID = get_run_id()\n",
    "\n",
    "RUN_TIME = datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(os.getenv('STORAGE_ACCOUNT_CONNECTION_STRING'))\n",
    "\n",
    "input_container_name = 'azure-openai-batch-processing-files'\n",
    "output_container_name = 'raw-articles-list'\n",
    "\n",
    "input_container = blob_service_client.get_container_client(input_container_name)\n",
    "assert input_container.exists(), f\"Input container '{input_container_name}' does not exist.\"\n",
    "output_container = blob_service_client.get_container_client(output_container_name)\n",
    "assert output_container.exists(), f\"Output container '{output_container_name}' does not exist.\"\n",
    "\n",
    "print(f\"Run ID: {RUNID} at {RUN_TIME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36bb02b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'RUNID_3--source_parsing_v0_OUTPUT_0.jsonl', 'container': 'azure-openai-batch-processing-files', 'snapshot': None, 'version_id': None, 'is_current_version': None, 'blob_type': <BlobType.BLOCKBLOB: 'BlockBlob'>, 'metadata': {}, 'encrypted_metadata': None, 'last_modified': datetime.datetime(2025, 6, 3, 8, 54, 7, tzinfo=datetime.timezone.utc), 'etag': '0x8DDA27C335CB9B3', 'size': 18692, 'content_range': None, 'append_blob_committed_block_count': None, 'is_append_blob_sealed': None, 'page_blob_sequence_number': None, 'server_encrypted': True, 'copy': {'id': None, 'source': None, 'status': None, 'progress': None, 'completion_time': None, 'status_description': None, 'incremental_copy': None, 'destination_snapshot': None}, 'content_settings': {'content_type': 'application/octet-stream', 'content_encoding': None, 'content_language': None, 'content_md5': bytearray(b'$Hz\\xe1o\\xef\\x14\\xe3\\xf2\\xd8\\xb9\\xc6\\xf4[\\xc9v'), 'content_disposition': None, 'cache_control': None}, 'lease': {'status': 'unlocked', 'state': 'available', 'duration': None}, 'blob_tier': 'Hot', 'rehydrate_priority': None, 'blob_tier_change_time': None, 'blob_tier_inferred': True, 'deleted': None, 'deleted_time': None, 'remaining_retention_days': None, 'creation_time': datetime.datetime(2025, 6, 3, 8, 54, 7, tzinfo=datetime.timezone.utc), 'archive_status': None, 'encryption_key_sha256': None, 'encryption_scope': None, 'request_server_encrypted': None, 'object_replication_source_properties': [], 'object_replication_destination_policy': None, 'last_accessed_on': None, 'tag_count': None, 'tags': None, 'immutability_policy': {'expiry_time': None, 'policy_mode': None}, 'has_legal_hold': None, 'has_versions_only': None}\n"
     ]
    }
   ],
   "source": [
    "for x in input_container.list_blobs(name_starts_with=f\"{RUNID}--{TASK_NAME}_OUTPUT\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "490eaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_outputs():\n",
    "    outputs = []\n",
    "    for blob_info in input_container.list_blobs(name_starts_with=f\"{RUNID}--{TASK_NAME}_OUTPUT\"):\n",
    "        blob_client = input_container.get_blob_client(blob_info.name)\n",
    "        lines_raw = blob_client.download_blob().readall().decode('utf-8').splitlines()\n",
    "        for line in lines_raw:\n",
    "            output_dict = json.loads(line)\n",
    "            model = output_dict.get(\"response\").get(\"body\").get(\"model\")\n",
    "            line_id = output_dict.get(\"custom_id\")\n",
    "            content_json = output_dict.get(\"response\").get(\"body\").get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "            content = json.loads(content_json)\n",
    "            outputs.append({\n",
    "                \"model\": model,\n",
    "                \"line_id\": line_id,\n",
    "                \"content\": content\n",
    "            })\n",
    "    return outputs\n",
    "\n",
    "def get_previously_crawled_article_titles():\n",
    "    previously_crawled_article_titles = []\n",
    "    for blob_info in output_container.list_blobs():\n",
    "        blob_name = blob_info.name\n",
    "        data = blob_service_client.get_blob_client(output_container_name, blob_name).download_blob().readall().decode('utf-8')\n",
    "        data = json.loads(data)\n",
    "        for item in data:\n",
    "            if 'article_title' in item:\n",
    "                previously_crawled_article_titles.append(item['article_title'])\n",
    "    return previously_crawled_article_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e9f88e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Opinion: Europe must warm up to geothermal before it’s too late as it has already been crawled.\n",
      "Skipping Bioprinted organs ‘10–15 years away,’ says startup regenerating dog skin as it has already been crawled.\n",
      "Skipping ‘Purest meat alternative’ to grow in Swedish mycoprotein factory as it has already been crawled.\n",
      "Skipping Elon Musk’s Grok chatbot banned by a quarter of European firms as it has already been crawled.\n",
      "Skipping ENISA ya cuenta con más de medio millar de startups certificadas as it has already been crawled.\n",
      "Skipping 7 metas que deberán marcar la relación de las empresas con la tecnología en 2024 as it has already been crawled.\n",
      "Skipping Apple es la compañía tecnológica que más ganancias tiene por empleado as it has already been crawled.\n",
      "Skipping Cómo mantener el impulso de tu startup durante la época navideña as it has already been crawled.\n",
      "Skipping Crecieron con smartphones, ahora optan por móviles 'tontos' as it has already been crawled.\n",
      "Skipping El CEO de Anthropic advierte de que la IA podría eliminar la mitad de los empleos de oficina as it has already been crawled.\n",
      "Skipping 'The New York Times' permitirá a Amazon usar sus artículos para entrenar modelos de IA as it has already been crawled.\n",
      "Skipping Elon Musk anuncia su salida del Gobierno de EEUU: \"Mi etapa como empleado especial ha terminado\" as it has already been crawled.\n",
      "Skipping Las 3 conclusiones de los resultados de Nvidia: China, China y China as it has already been crawled.\n",
      "Skipping Así están convirtiendo la ansiedad por la IA en una ventaja los equipos de contratación as it has already been crawled.\n",
      "Skipping Meta, la matriz de Facebook, planea abrir tiendas físicas para vender sus gafas inteligentes as it has already been crawled.\n",
      "Skipping Palo sideral a Elon Musk: esto es lo que cuesta el megacohete Starship que ha explotado sobre el Índico as it has already been crawled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RawArticle(BaseModel):\n",
    "    model: str\n",
    "    run_id: str\n",
    "    task_name: str\n",
    "    source_name: str\n",
    "    article_id: str\n",
    "    article_title: str\n",
    "    article_url: str\n",
    "    article_keywords: list[str]\n",
    "    article_language: str\n",
    "    crawled_at: str\n",
    "\n",
    "\n",
    "previously_crawled_article_titles = get_previously_crawled_article_titles()\n",
    "\n",
    "new_raw_articles_list = []\n",
    "for output in read_outputs():\n",
    "    model = output['model']\n",
    "    line_id = output['line_id']\n",
    "    run_id, task_name, source_name = line_id.split(\"--\")\n",
    "    content = output['content']\n",
    "\n",
    "    article_links_list = content.get(\"article_links_list\", [])\n",
    "    for article_link in article_links_list:\n",
    "        article_title = article_link.get(\"title\", \"\")\n",
    "        article_url = article_link.get(\"url\", \"\")\n",
    "        article_keywords = article_link.get(\"keywords\", [])\n",
    "        article_language = article_link.get(\"language\", \"\")\n",
    "        article_id = source_name + \"_\" + datetime.now().strftime('%Y%m%d%H%M%S%f')\n",
    "\n",
    "        sleep(0.01)\n",
    "\n",
    "        if article_title in previously_crawled_article_titles:\n",
    "            print(f\"Skipping {article_title} as it has already been crawled.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            raw_article = RawArticle(\n",
    "                model=model,\n",
    "                run_id=run_id,\n",
    "                task_name=task_name,\n",
    "                source_name=source_name,\n",
    "                article_id=article_id,\n",
    "                article_title=article_title,\n",
    "                article_url=article_url,\n",
    "                article_keywords=article_keywords,\n",
    "                article_language=article_language,\n",
    "                crawled_at=RUN_TIME\n",
    "            )\n",
    "            new_raw_articles_list.append(raw_article.model_dump())\n",
    "        except ValidationError as e:\n",
    "            print(f\"Validation error for article '{article_title}'\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a0c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(new_raw_articles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68e9ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_articles_list():\n",
    "    output_blob_name = f\"{RUNID}--raw_articles_list.json\"\n",
    "    output_blob_client = output_container.get_blob_client(output_blob_name)\n",
    "    output_blob_client.upload_blob(json.dumps(new_raw_articles_list, indent=4), overwrite=True)\n",
    "    print(f\"Raw articles list saved to blob storage as {output_blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11bc82a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw articles list saved to blob storage as RUNID_3--raw_articles_list.json\n"
     ]
    }
   ],
   "source": [
    "save_raw_articles_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50cc741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
