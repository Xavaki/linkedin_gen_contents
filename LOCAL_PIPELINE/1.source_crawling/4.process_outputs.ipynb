{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0288e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "from pydantic import BaseModel, ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98b83927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74a06986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: RUNID_2 at 2025-05-30 14:47:55\n"
     ]
    }
   ],
   "source": [
    "TASK_NAME = \"source_parsing_v0\"\n",
    "\n",
    "\n",
    "def get_run_id():\n",
    "    return os.getenv('RUNID') \n",
    "\n",
    "RUNID = get_run_id()\n",
    "\n",
    "INPUT_DATA_PATH = f\"../local_tests_data/azure_openai_batch_processing_files/{RUNID}/{TASK_NAME}/OUTPUTS/\"\n",
    "\n",
    "RUN_TIME = datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "OUTPUT_DATA_PATH = f\"../local_tests_data/raw_articles_list/{RUNID}/\"\n",
    "\n",
    "os.makedirs(OUTPUT_DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Run ID: {RUNID} at {RUN_TIME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "490eaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_outputs():\n",
    "    outputs = []\n",
    "    for filename in os.listdir(INPUT_DATA_PATH):\n",
    "        if filename.endswith('.jsonl'):\n",
    "            with open(os.path.join(INPUT_DATA_PATH, filename), 'r') as f:\n",
    "                    lines_raw = f.readlines()\n",
    "                    for line in lines_raw:\n",
    "                        output_dict = json.loads(line)\n",
    "                        model = output_dict.get(\"response\").get(\"body\").get(\"model\")\n",
    "                        line_id = output_dict.get(\"custom_id\")\n",
    "                        content_json = output_dict.get(\"response\").get(\"body\").get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "                        content = json.loads(content_json)\n",
    "                        outputs.append({\n",
    "                            \"model\": model,\n",
    "                            \"line_id\": line_id,\n",
    "                            \"content\": content\n",
    "                        })\n",
    "    return outputs\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_previously_crawled_article_titles():\n",
    "\n",
    "    root_dir = Path(OUTPUT_DATA_PATH).parent\n",
    "    pattern = '*.json'               \n",
    "\n",
    "    search_path = os.path.join(root_dir, '**', pattern)\n",
    "    all_filenames = glob.glob(search_path, recursive=True)\n",
    "\n",
    "    previously_crawled_article_titles = []\n",
    "    for filepath in all_filenames:\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            for item in data:\n",
    "                if 'article_title' in item:\n",
    "                    previously_crawled_article_titles.append(item['article_title'])\n",
    "\n",
    "    return previously_crawled_article_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e9f88e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Bioprinted organs ‘10–15 years away,’ says startup regenerating dog skin as it has already been crawled.\n",
      "Skipping ‘Purest meat alternative’ to grow in Swedish mycoprotein factory as it has already been crawled.\n",
      "Skipping Can geothermal startups drill Europe to clean energy independence? as it has already been crawled.\n",
      "Skipping TNW Backstage tunes into the future of earbuds as it has already been crawled.\n",
      "Skipping ENISA ya cuenta con más de medio millar de startups certificadas as it has already been crawled.\n",
      "Skipping 7 metas que deberán marcar la relación de las empresas con la tecnología en 2024 as it has already been crawled.\n",
      "Skipping Apple es la compañía tecnológica que más ganancias tiene por empleado as it has already been crawled.\n",
      "Skipping Cómo mantener el impulso de tu startup durante la época navideña as it has already been crawled.\n",
      "Skipping Google paga 5.000 millones de dólares para resolver una demanda colectiva as it has already been crawled.\n",
      "Skipping Meta, la matriz de Facebook, planea abrir tiendas físicas para vender sus gafas inteligentes as it has already been crawled.\n",
      "Skipping Palo sideral a Elon Musk: esto es lo que cuesta el megacohete Starship que ha explotado sobre el Índico as it has already been crawled.\n",
      "Skipping El ecosistema tecnológico español crece un 22% y Madrid desplaza a Barcelona como capital de la innovación as it has already been crawled.\n",
      "Skipping Las ventas de Tesla siguen desplomándose en Europa pese al crecimiento de los coches eléctricos as it has already been crawled.\n",
      "Skipping Securitas Direct ultima su salida a Bolsa con una valoración superior a los 20.000 millones de euros as it has already been crawled.\n",
      "Skipping El CEO de Duolingo revela 5 formas en las que la IA le ayudará a decidir el futuro de su plantilla as it has already been crawled.\n",
      "Skipping Esta es la historia jamás contada de cómo la startup de programación más prometedora de Silicon Valley estuvo a punto de desaparecer as it has already been crawled.\n",
      "Skipping 335 millones menos en estafas: cómo Visa bloqueó el 75% de las tarjetas comprometidas en una operación criminal con inteligencia artificial as it has already been crawled.\n",
      "Skipping Así lograron las compañías de medios tradicionales la rentabilidad en el streaming: menos Netflix, más ADN propio as it has already been crawled.\n",
      "Skipping Instagram ofrece pagar a los creadores de contenido hasta 18.000 euros por atraer personas a la aplicación as it has already been crawled.\n",
      "Skipping Brian Chesky (Airbnb) cree que hay un \"lado positivo\" en montar negocios en una economía inestable as it has already been crawled.\n",
      "Skipping MF Talent premia el talento y la innovación e impulsa la IA y la ciberseguridad como motores de un futuro más sostenible as it has already been crawled.\n",
      "Skipping Xiaomi lanza su segundo coche eléctrico: su YU7 es la nueva amenaza para Tesla en China as it has already been crawled.\n",
      "Skipping Cook ignora a Trump: Apple ampliará su cadena de suministro en India con una planta de 1.300 millones de euros as it has already been crawled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RawArticle(BaseModel):\n",
    "    model: str\n",
    "    run_id: str\n",
    "    task_name: str\n",
    "    source_name: str\n",
    "    article_id: str\n",
    "    article_title: str\n",
    "    article_url: str\n",
    "    article_keywords: list[str]\n",
    "    article_language: str\n",
    "    crawled_at: str\n",
    "\n",
    "\n",
    "previously_crawled_article_titles = get_previously_crawled_article_titles()\n",
    "\n",
    "new_raw_articles_list = []\n",
    "for output in read_outputs():\n",
    "    model = output['model']\n",
    "    line_id = output['line_id']\n",
    "    run_id, task_name, source_name = line_id.split(\"--\")\n",
    "    content = output['content']\n",
    "\n",
    "    article_links_list = content.get(\"article_links_list\", [])\n",
    "    for article_link in article_links_list:\n",
    "        article_title = article_link.get(\"title\", \"\")\n",
    "        article_url = article_link.get(\"url\", \"\")\n",
    "        article_keywords = article_link.get(\"keywords\", [])\n",
    "        article_language = article_link.get(\"language\", \"\")\n",
    "        article_id = source_name + \"_\" + datetime.now().strftime('%Y%m%d%H%M%S%f')\n",
    "\n",
    "        sleep(0.01)\n",
    "\n",
    "        if article_title in previously_crawled_article_titles:\n",
    "            print(f\"Skipping {article_title} as it has already been crawled.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            raw_article = RawArticle(\n",
    "                model=model,\n",
    "                run_id=run_id,\n",
    "                task_name=task_name,\n",
    "                source_name=source_name,\n",
    "                article_id=article_id,\n",
    "                article_title=article_title,\n",
    "                article_url=article_url,\n",
    "                article_keywords=article_keywords,\n",
    "                article_language=article_language,\n",
    "                crawled_at=RUN_TIME\n",
    "            )\n",
    "            new_raw_articles_list.append(raw_article.model_dump())\n",
    "        except ValidationError as e:\n",
    "            print(f\"Validation error for article '{article_title}'\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0a0c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(new_raw_articles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68e9ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_articles_list():\n",
    "    with open(os.path.join(OUTPUT_DATA_PATH, f\"raw_articles_list_{RUNID}.json\"), 'w') as f:\n",
    "        json.dump(new_raw_articles_list, f, indent=4)\n",
    "        print(f\"New raw articles list saved to {OUTPUT_DATA_PATH}raw_articles_list_{RUNID}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11bc82a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New raw articles list saved to ../local_tests_data/raw_articles_list/RUNID_2/raw_articles_list_RUNID_2.json\n"
     ]
    }
   ],
   "source": [
    "save_raw_articles_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50cc741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
